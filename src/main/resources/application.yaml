spring:
  spring:
    ai:
      ollama:
        base-url: http://localhost:11434           # default Ollama server
        chat:
          options:
            model: deepseek-coder-v2:16b         # your downloaded model
            temperature: 0.7                     # 0.0 = deterministic, 1.0 = creative
            max-tokens: 1024                     # max output length
  threads:
    virtual:
      enabled: true
  application:
    name: demo
